{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2.1:\n",
    "In \"epsilon-greedy action selection, for the case of two actions and \" =0.5, what is the probability that the greedy action is selected? \n",
    "\n",
    "As when taking random actions, we might actually select optimal action as well. Therefore, the probability is 0.5 + 0.5 * 0.5  = 0.75\n",
    "\n",
    "## Ex 2.2\n",
    "Consider a k-armed bandit problem with k = 4 actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using \"epsilon-greedy action selection, sample-average action-value estimates, and initial estimates of Q1(a) = 0, for all a. Suppose the initial sequence of actions and rewards is A1 = 1, R1 = -1, A2 = 2, R2 = 1, A3 = 2, R3 = -2, A4 = 2, R4 = 2, A5 = 3, R5 = 0. On some of these time steps the \" case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?\n",
    "\n",
    "Step 1: A1 = 1, R1 = -1: Random selection/greedy with breaking ties among (1, 2, 3, 4). Greedy action is (2,3,4). (Maybe)\n",
    "\n",
    "Step 2: A2 = 2, R2 = 1:  Possible greedy/ random selection among (2, 3, 4). Greedy aciton is (2) (Maybe)\n",
    "\n",
    "Step 3: A3 = 2, R3 = -2: Possible greedy/random. Greedy action is (2) (Maybe)\n",
    "\n",
    "Step 4: A4 = 2, R4 = 2: Possible greedy/random. Greedy action is (2) (Maybe)\n",
    "\n",
    "Step 5: A5 = 3, R5 = 0: Random because greedy action is (2) (Yes)\n",
    "\n",
    "## Ex 2.3\n",
    "In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively.\n",
    "\n",
    "epsilon-greedy with epsilon = 0.01 will perform best.\n",
    "Best cumulative reward = (1-epsilon + epsilon/number_of_possible_action)*(q*) + epsilon/ number_of_possible_action * average of other non-optimal actions\n",
    "\n",
    "Best probability of selecting the best action: 1 - epsilon + epsilon / (number of possible action)\n",
    "\n",
    "## Ex 2.4\n",
    "If the step-size parameters, alpha_n, are not constant, then the estimate Qn is a weighted average of previously received rewards with a weighting different from that given by (2.6). What is the weighting on each prior reward for the general case, analogous to (2.6), in terms of the sequence of step-size parameters?\n",
    "\n",
    "$Q_{n+1} = Q_1 \\prod_{i=1}^{n}(1-\\alpha_i) + \\sum_{i=1}^n R_i \\alpha_i \\prod_{j=i}^{n-1}(1-\\alpha_{j+1})$\n",
    "Weighting for $R_i$: $\\alpha_i \\prod_{j=i}^{n-1}(1 - \\alpha_{j+1})$ with $i \\leq n - 1$\n",
    "Weighting for $R_n$: $\\alpha_n$\n",
    "\n",
    "## Ex 2.6\n",
    "Mysterious Spikes The results shown in Figure 2.3 should be quite reliable because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks. Why, then, are there oscillations and spikes in the early part of the curve for the optimistic method? In other words, what might make this method perform particularly better or worse, on average, on particular early steps?\n",
    "\n",
    "As there is an optimistic initial values, the agent will try all possible actions at the beginning and it will eventually select the optimal actions. Selecting the optimal action will return high reward values and the agent will then keep selecting the optimal actions until \n",
    "\n",
    "## Ex 2.9\n",
    "Show that in the case of two actions, the soft-max distribution is the same as that given by the logistic, or sigmoid, function often used in statistics and artificial neural networks.\n",
    "\n",
    "For action 1 (similar to action 2), you can convert from softmax -> sigmoid:\n",
    "\n",
    "$p\\{A_t = a_1\\} =  \\frac{e^{H_t(a_1)}}{e^{H_t(a_1)} + e^{H_t(a_2)}} = \\frac{1}{1 + e^{-(H_t(a_1) - H_t(a_2))}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
