{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1.1\n",
    "Self-Play:\n",
    "This is what Alpha Zero was trained. In this case, I think the agent will improve itself in terms of both attacking and defense strategies by self-corrections. It will learn a different policy as the opponent is now itself with a different way of playing. Eventually, when the agent becomes much better, the game will most likely lead to a draw because there might be no weakness to explored. However, the learning might be more challenging as the opponent (itself) might change ways of playing over time.\n",
    "\n",
    "## Ex 1.2\n",
    "Symmetries:\n",
    "We might take advantage of the symmetry by have a different way of representing states to reduce the state space. As many combinations of action + previous positions of the board will lead to the same board configurations, we can just use this final configurations to be the state. As the state space is less, the learning process my become better. \n",
    "\n",
    "## Ex 1.3\n",
    "Greedy play:\n",
    "If the RL player plays purely greedy, it only exploits. As a result, the policy might converge to sub-optimal policies. It will perform worse than a non-greedy player who has room for exploring the environment and might discover better strategies. The problems include: converging to sub-optimal policies as a result of incorrectly estimations of the state value, worse performance.\n",
    "\n",
    "## Ex 1.4\n",
    "Learning from exploration: \n",
    "Exploration might bring 2 benefits:\n",
    "1. It might lead to states that have high rewards.\n",
    "2. If we also update the values of states prior exploration moves, we can have more accurate estimates of these states.\n",
    "<br/>Therefore, learning from all moves (including exploratory moves) should be preferred as it might bring both benefits. \n",
    "As a result, playing in this way should bring more wins.\n",
    "\n",
    "## Ex 1.5\n",
    "Other improvements:\n",
    "1. Learning from all moves\n",
    "2. Reducing step-size over time to reducing less exploration near convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
